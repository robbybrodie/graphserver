apiVersion: batch/v1
kind: Job
metadata:
  name: schema-migration-job
  namespace: graphserver
  annotations:
    argocd.argoproj.io/sync-wave: "1"  # Run before ETL jobs
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  template:
    metadata:
      labels:
        app: schema-migration
        component: company-specific
    spec:
      restartPolicy: OnFailure
      containers:
      - name: schema-migration
        image: neo4j:5.15-community
        command:
        - /bin/bash
        - -c
        - |
          echo "Starting schema migration process..."
          
          # Check current schema version
          CURRENT_VERSION=$(cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
            "MATCH (v:SchemaVersion) RETURN v.version ORDER BY v.applied DESC LIMIT 1;" \
            --format plain 2>/dev/null | tail -n +2 | head -n 1 || echo "0")
          
          echo "Current schema version: ${CURRENT_VERSION}"
          
          # Target version from ConfigMap
          TARGET_VERSION=$(cat /migrations/target-version.txt)
          echo "Target schema version: ${TARGET_VERSION}"
          
          if [ "${CURRENT_VERSION}" = "${TARGET_VERSION}" ]; then
            echo "Schema is already at target version ${TARGET_VERSION}"
            exit 0
          fi
          
          # Backup existing data before migration
          echo "Creating data backup before migration..."
          cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
            "CALL apoc.export.cypher.all('/tmp/backup-pre-migration.cypher', {});" || {
            echo "WARNING: Backup failed, continuing with migration..."
          }
          
          # Apply migration scripts in order
          for migration_file in /migrations/migrations/*.cypher; do
            if [ -f "$migration_file" ]; then
              migration_version=$(basename "$migration_file" .cypher | sed 's/^[0-9]*-//')
              echo "Applying migration: $migration_file"
              
              # Apply migration with error handling
              if cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
                --file "$migration_file"; then
                echo "Migration $migration_file applied successfully"
                
                # Record migration in database
                cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
                  "CREATE (v:SchemaVersion {version: '${TARGET_VERSION}', applied: datetime(), migration: '$(basename $migration_file)'});"
              else
                echo "ERROR: Migration $migration_file failed"
                
                # Attempt rollback if backup exists
                if [ -f "/tmp/backup-pre-migration.cypher" ]; then
                  echo "Attempting rollback from backup..."
                  cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
                    "MATCH (n) DETACH DELETE n;" || true
                  cypher-shell -a bolt://neo4j:7687 -u neo4j -p ${NEO4J_PASSWORD} \
                    --file "/tmp/backup-pre-migration.cypher" || true
                fi
                exit 1
              fi
            fi
          done
          
          echo "Schema migration completed successfully to version ${TARGET_VERSION}"
        env:
        - name: NEO4J_PASSWORD
          valueFrom:
            secretKeyRef:
              name: neo4j-auth
              key: NEO4J_PASSWORD
        volumeMounts:
        - name: migrations
          mountPath: /migrations
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
      volumes:
      - name: migrations
        configMap:
          name: schema-migrations
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: schema-migrations
  namespace: graphserver
  annotations:
    argocd.argoproj.io/sync-wave: "0"  # Deploy before migration job
data:
  target-version.txt: "2.0"
  
  # Migration scripts - applied in alphabetical order
  migrations/001-add-repository-categories.cypher: |
    // Migration 001: Add repository categories and enhanced indexing
    // Safe to run multiple times (idempotent)
    
    // Add new indexes for enhanced performance
    CREATE INDEX github_issue_org IF NOT EXISTS FOR (g:GitHubIssue) ON (g.organization);
    CREATE INDEX github_issue_type IF NOT EXISTS FOR (g:GitHubIssue) ON (g.type);
    CREATE INDEX component_category IF NOT EXISTS FOR (c:Component) ON (c.category);
    CREATE INDEX technology_category IF NOT EXISTS FOR (t:Technology) ON (t.category);
    
    // Add repository category property to existing GitHubIssue nodes
    MATCH (g:GitHubIssue)
    WHERE g.category IS NULL
    SET g.category = CASE 
      WHEN g.repository CONTAINS "ansible-core" OR g.repository CONTAINS "/ansible" THEN "core"
      WHEN g.repository CONTAINS "awx" OR g.repository CONTAINS "channels" THEN "controller"
      WHEN g.repository CONTAINS "galaxy" OR g.repository CONTAINS "automation-hub" OR g.repository CONTAINS "collections" THEN "collections"
      WHEN g.repository CONTAINS "receptor" THEN "networking"
      WHEN g.repository CONTAINS "builder" OR g.repository CONTAINS "execution-environment" OR g.repository CONTAINS "navigator" THEN "execution_environments"
      WHEN g.repository CONTAINS "runner" THEN "runner"
      WHEN g.repository CONTAINS "molecule" OR g.repository CONTAINS "lint" OR g.repository CONTAINS "test" OR g.repository CONTAINS "ara" THEN "cicd"
      WHEN g.repository CONTAINS "dev-tools" OR g.repository CONTAINS "metadata" THEN "developer_tools"
      WHEN g.repository CONTAINS "event-driven" OR g.repository CONTAINS "eda" THEN "event_driven"
      WHEN g.repository CONTAINS "insights" THEN "insights"
      ELSE "other"
    END;
    
    // Create Component nodes for new categories if they don't exist
    MERGE (c1:Component {name: "execution_environments", category: "infrastructure"})
    MERGE (c2:Component {name: "runner", category: "infrastructure"})
    MERGE (c3:Component {name: "developer_tools", category: "tooling"})
    MERGE (c4:Component {name: "event_driven", category: "platform"})
    MERGE (c5:Component {name: "insights", category: "platform"});

  migrations/002-add-custom-procedures.cypher: |
    // Migration 002: Add custom analytical procedures
    // Safe to run multiple times (procedures are replaced)
    
    // Repository ecosystem analysis procedure
    CALL apoc.custom.asProcedure(
      'analyzeRepositoryEcosystem',
      'MATCH (g:GitHubIssue)
       WHERE g.organization IN ["ansible", "ansible-collections", "ansible-community", "RedHatInsights"]
       WITH g.repository as repo, 
            count(g) as totalIssues,
            count(case when g.state = "open" then 1 end) as openIssues,
            max(g.updated) as lastActivity,
            coalesce(g.category, "other") as category
       RETURN collect({
         repository: repo,
         category: category,
         totalIssues: totalIssues,
         openIssues: openIssues,
         closedIssues: totalIssues - openIssues,
         lastActivity: lastActivity,
         activityScore: case when lastActivity > datetime() - duration("P30D") then "high"
                             when lastActivity > datetime() - duration("P90D") then "medium"
                             else "low" end
       }) as repositoryEcosystem',
      'read',
      [['repositoryEcosystem','LIST OF MAP']]
    );
    
    // Cross-ecosystem impact analysis procedure
    CALL apoc.custom.asProcedure(
      'analyzeCrossEcosystemImpact',
      'MATCH (j:JiraIssue)
       WHERE j.project IN ["ANSTRAT", "AAPRFE"] AND j.status IN ["Open", "In Progress", "New"]
       OPTIONAL MATCH (j)-[:TRACKED_IN|IMPLEMENTED_BY]->(g:GitHubIssue)
       WITH j, collect(coalesce(g.category, "other")) as impactedCategories
       UNWIND j.components as component
       WITH component, 
            count(j) as strategicItems,
            apoc.coll.flatten(collect(impactedCategories)) as allCategories
       RETURN collect({
         component: component,
         strategicItems: strategicItems,
         impactedCategories: apoc.coll.toSet(allCategories),
         ecosystemBreadth: size(apoc.coll.toSet(allCategories))
       }) as crossEcosystemImpact',
      'read',
      [['crossEcosystemImpact','LIST OF MAP']]
    );

  migrations/003-data-quality-improvements.cypher: |
    // Migration 003: Data quality improvements and cleanup
    // Safe to run multiple times
    
    // Remove duplicate relationships
    MATCH (j:JiraIssue)-[r1:TRACKED_IN]->(g:GitHubIssue)
    MATCH (j)-[r2:TRACKED_IN]->(g)
    WHERE id(r1) > id(r2)
    DELETE r2;
    
    MATCH (g:GitHubIssue)-[r1:ADDRESSES]->(j:JiraIssue)
    MATCH (g)-[r2:ADDRESSES]->(j)
    WHERE id(r1) > id(r2)
    DELETE r2;
    
    // Add missing lastSynced properties
    MATCH (j:JiraIssue)
    WHERE j.lastSynced IS NULL
    SET j.lastSynced = datetime();
    
    MATCH (g:GitHubIssue)
    WHERE g.lastSynced IS NULL
    SET g.lastSynced = datetime();
    
    // Standardize status values
    MATCH (j:JiraIssue)
    WHERE j.status IN ["Done", "Resolved", "Fixed"]
    SET j.status = "Closed";
    
    // Add organization property to GitHub issues if missing
    MATCH (g:GitHubIssue)
    WHERE g.organization IS NULL AND g.repository IS NOT NULL
    SET g.organization = split(g.repository, "/")[0];

  rollback-instructions.md: |
    # Schema Migration Rollback Instructions
    
    ## Automatic Rollback
    The migration job includes automatic rollback on failure using APOC export/import.
    
    ## Manual Rollback Process
    
    1. **Stop ETL processes:**
    ```bash
    kubectl scale deployment jira-github-integration-etl --replicas=0 -n graphserver
    ```
    
    2. **Connect to Neo4j:**
    ```bash
    kubectl port-forward svc/neo4j 7687:7687 -n graphserver
    ```
    
    3. **Check available backups:**
    ```cypher
    CALL apoc.systemdb.execute("SHOW FILES");
    ```
    
    4. **Restore from backup:**
    ```cypher
    // Clear current data
    MATCH (n) DETACH DELETE n;
    
    // Import backup
    CALL apoc.cypher.runFile('/tmp/backup-pre-migration.cypher');
    ```
    
    5. **Restart ETL processes:**
    ```bash
    kubectl scale deployment jira-github-integration-etl --replicas=1 -n graphserver
    ```
    
    ## Prevention
    - Always test migrations in development environment first
    - Use feature flags for gradual rollout
    - Monitor data integrity after migrations
